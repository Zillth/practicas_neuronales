{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso por gradiente completo\n",
    "\n",
    "En este notebook implementaremos el algoritmo completo de descenso por gradiente. Para validar que funciona al final lo probaremos en el entrenamiento de una red neuronal simple. Usaremos como conjunto de datos que esta incluído en el archivo data.csv. \n",
    "\n",
    "@juan1rving\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos los paquete necesarios\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# cargamos datos\n",
    "data = pd.read_csv('data/data.csv', sep=',', engine='python')\n",
    "data['gre_scaled'] = data['gre'] / 1000\n",
    "data['gpa_scaled'] = data['gpa'] / 10\n",
    "data['rank_scaled'] = data['rank'] / 10\n",
    "X = data[['gre_scaled', 'gpa_scaled']]\n",
    "y = data['admit']\n",
    "n_records, n_features = X.shape\n",
    "\n",
    "# Dividimos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "last_loss = None\n",
    "\n",
    "# En este ejercicio por propósitos de analizar las salidas utilizaremos la misma semilla para los números aleatorios.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos algunas funciones necesarias\n",
    "# función de activación\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivada de f\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# función h lineal\n",
    "def function_h(X, W, b):\n",
    "    return np.dot(W, X) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización de los pesos\n",
    "\n",
    "En un principio no queremos tener todos los pesos en cero porque esto generaría en la salida una predicción nula. Por lo tanto, asignaremos los pesos iniciales de forma aleatoria y cercanos a cero. Otra recomendación es escalar los valores aleatorios es dependencia del número de entradas del nodo (n).\n",
    "\n",
    "$$w = rand(1,\\frac{1}{\\sqrt{n}})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize weights. \n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "bias = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.338\n"
     ]
    }
   ],
   "source": [
    "# Probemos la precisión de la red antes de entrenarla\n",
    "test_out = [sigmoid(function_h(x, weights, bias)) > 0.5 for x in X_test.values]\n",
    "accuracy = accuracy_score(y_test, test_out)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n",
    "\n",
    "# La precisión debe ser mala seguramente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros de la red\n",
    "\n",
    "Los hiperpámetros de la red indican el números de veces que itera el método (épocas-epochs), la taza de aprendizaje (learning rate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# número de épocas\n",
    "epochs = 2\n",
    "# tasa de aprendizaje\n",
    "learnrate = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descenso por gradiente completo.\n",
    "\n",
    "El algoritmo de descenso por gradiente de forma iterativa cambia el valor de los pesos de tal forma que se disminuya el error. \n",
    "\n",
    "<img src=\"files/nn_02_simple_nn_dg.png\">\n",
    "\n",
    "En la siguiete celda encontrarás la plantilla del algoritmo. Tu misión, si decides aceptarla, es completar el código faltante para que funcione el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.30016790491702905\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    delta_w = np.zeros(weights.shape)\n",
    "    # Para todos los renglones de ejemplo, asignar a x la entrada, y a y la salida deseada\n",
    "    for x, y in zip(X_train.values, y_train.values):\n",
    "        # TODO: calcula la predicción de la red\n",
    "        # Tip: NumPy ya tiene una función que calcula el producto punto. Recuerda que también los logits tienen que pasar por la función de activación.\n",
    "        h = function_h(x, weights, bias)\n",
    "        output = sigmoid(h)\n",
    "\n",
    "        # TODO: calcula el error\n",
    "        error = np.multiply((y - output), sigmoid_prime(h))\n",
    "\n",
    "        # TODO: calcula el incremento\n",
    "        delta_w += np.multiply(learnrate, np.multiply(error, x))\n",
    "        \n",
    "    # TODO: Actualiza los pesos\n",
    "    weights += delta_w\n",
    "    \n",
    "    # Ahora calculemos el error en el conjunto de datos de entrenamiento\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = [sigmoid(function_h(x, weights, bias)) for x in X_train.values]\n",
    "        loss = np.mean((out - y_train) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluemos la exactitud de la red\n",
    "\n",
    "Recordemos que\n",
    "\n",
    "$$ Exactitud = \\frac{\\# aciertos}{\\# predicciones} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model train accuracy: 0.688\n",
      "Model test accuracy: 0.662\n"
     ]
    }
   ],
   "source": [
    "# Cálculo de la precisión\n",
    "# Cálculo de la precisión (train)\n",
    "test_out = [sigmoid(function_h(x, weights, bias)) > 0.5 for x in X_train.values]\n",
    "accuracy = accuracy_score(y_train, test_out)\n",
    "print(\"Model train accuracy: {:.3f}\".format(accuracy))\n",
    "\n",
    "# Cálculo de la precisión (test)\n",
    "test_out = [sigmoid(function_h(x, weights, bias)) > 0.5 for x in X_test.values]\n",
    "accuracy = accuracy_score(y_test, test_out)\n",
    "print(\"Model test accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal vez obtuvimos la mejor exactidud, pero ¿qué pasará si incrementamos las épocas? O ¿qué es lo que pasará si cambiamos la tasa de aprendizaje?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbad788490f55b163920bee5e9d5e0cba00db5905dc94f4bdbe0011e55bf465f"
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
